{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZHUhCEE7bfFs3/CnxbKMj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WekSdgUja_Bj"},"outputs":[],"source":["import warnings warnings.filterwarnings(\"ignore\") import matplotlib.pyplot as plt import numpy as np\n","import librosa\n","import IPython.display as ipd from scipy.io import wavfile\n","# load audio dataset from kaggle\n","train_audio_path = '/kaggle/working/tensorflow-speech-recognition-challenge/train/train/audio/'\n","# find the sample rate\n","sample_rate=40000\n","samples, sample_rate = librosa.load(train_audio_path+'yes/0a7c2a8d_nohash_0.wav', sr = sample_rate)\n","ipd.Audio(samples, rate=sample_rate)\n","sample_rate=16000\n","samples, sample_rate = librosa.load(train_audio_path+'yes/0a7c2a8d_nohash_0.wav', sr = sample_rate)\n","ipd.Audio(samples, rate=sample_rate)\n","# resampling\n","samples = librosa.resample(samples, sample_rate, 8000) ipd.Audio(samples, rate=8000) labels=os.listdir(train_audio_path)\n","no_of_recordings=[]\n","for label in labels:\n","waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')] no_of_recordings.append(len(waves))\n","#plot\n","plt.figure(figsize=(30,5))\n","index = np.arange(len(labels))\n","plt.bar(index, no_of_recordings) plt.xlabel('Commands', fontsize=12) plt.ylabel('No of recordings', fontsize=12) plt.xticks(index, labels, fontsize=15, rotation=60) plt.title('No. of recordings for each command') plt.show()\n","duration_of_recordings=[]\n","for label in labels:\n","waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]"]},{"cell_type":"code","source":["for wav in waves:\n","sample_rate, samples = wavfile.read(train_audio_path + '/' + label + '/' + wav) duration_of_recordings.append(float(len(samples)/sample_rate))\n","plt.hist(np.array(duration_of_recordings))\n","all_wave = []\n","all_label = []\n","for label in labels:\n","print(label)\n","waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n","for wav in waves:\n","samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000) samples = librosa.resample(samples, sample_rate, 8000)\n","if(len(samples)== 8000) : all_wave.append(samples) all_label.append(label)\n","from sklearn.preprocessing import LabelEncoder le = LabelEncoder() y=le.fit_transform(all_label)\n","classes= list(le.classes_)\n","# split dataset\n","from sklearn.model_selection import train_test_split\n","x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)\n","# Designing a model\n","from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D from keras.models import Model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","K.clear_session()\n","inputs = Input(shape=(8000,1))\n","#First Conv1D layer\n","conv = Conv1D(filters=8,kernel_size=13, padding='valid', activation='relu', strides=1)(inputs) conv = MaxPooling1D(3)(conv)\n","conv = Dropout(0.3)(conv)\n","#Second Conv1D layer\n","conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv) conv = MaxPooling1D(3)(conv)\n","conv = Dropout(0.3)(conv)\n","#Third Conv1D layer\n","conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv) conv = MaxPooling1D(3)(conv)"],"metadata":{"id":"c-h3z2hEbOTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv = Dropout(0.3)(conv)\n","#Fourth Conv1D layer\n","conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv) conv = MaxPooling1D(3)(conv)\n","conv = Dropout(0.3)(conv)\n","#Flatten layer\n","conv = Flatten()(conv)\n","#Dense Layer 1\n","conv = Dense(256, activation='relu')(conv) conv = Dropout(0.3)(conv)\n","#Dense Layer 2\n","conv = Dense(128, activation='relu')(conv) conv = Dropout(0.3)(conv)\n","outputs = Dense(len(labels), activation='softmax')(conv)\n","model = Model(inputs, outputs) model.summary()\n","# Compile & evaluate model\n","model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))\n","#predict text for given audio\n","import speech_recognition as sr r = sr.Recognizer()\n","with sr.Microphone() as source:\n","print(\"Speak Anything :\") audio = r.listen(source) try:\n","text = r.recognize_google(audio)\n","print(\"You said : {}\".format(text)) except:\n","print(\"Sorry could not recognize what you said\")"],"metadata":{"id":"f--JCpQWbSI8"},"execution_count":null,"outputs":[]}]}